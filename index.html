<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</h1>
        <div class="is-size-5 publication-authors">
          <span class="author-block"><a href="#" target="_blank"><b>Tao Lin</b></a><sup>1,2,3</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Yilei Zhong</b></a><sup>1</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Yuxin Du</b></a><sup>1,2,3</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Jingjing Zhang</b></a><sup>1</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Jiting Liu</b></a><sup>1</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Yinxinyu Chen</b></a><sup>5</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Encheng Gu</b></a><sup>6</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Ziyan Liu</b></a><sup>1</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Hongyi Cai</b></a><sup>1</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Yanwen Zou</b></a><sup>1,3,4</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Lixing Zou</b></a><sup>1</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Zhaoye Zhou</b></a><sup>1</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Gen Li</b></a><sup>1,3,7†</sup>,</span>
          <span class="author-block"><a href="#" target="_blank"><b>Bo Zhao</b></a><sup>1,2,3†</sup></span>
        </div>


                <div class="is-size-5 publication-authors">
                  <span class="author-block">
                    <sup>1</sup>School of AI, Shanghai Jiao Tong University, 
                    <sup>2</sup>EvoMind Tech, 
                    <sup>3</sup>IAAR-Shanghai,
                    <sup>4</sup>SII
                    <sup>5</sup>Carnegie Mellon University, 
                    <sup>6</sup>University of Cambridge,
                    <sup>7</sup>Nanyang Technological University                   
                    <br>
                  </span>
                  <span class="eql-cntrb">
                    <small><br><sup>†</sup>Corresponding Author</small>
                  </span>
                </div>


                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="static/pdfs/2511.04555v1.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/MINT-SJTU/Evo-1" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://www.arxiv.org/abs/2511.04555" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Vision-Language-Action (VLA) models have emerged as a
powerful framework that unifies perception, language, and
control, enabling robots to perform diverse tasks through
multimodal understanding. However, current VLA models
typically contain massive parameters and rely heavily on
large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training
paradigms often degrade the perceptual representations of
the vision-language backbone, resulting in overfitting and
poor generalization to downstream tasks. In this work, we
present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot
data. Evo-1 builds on a native multimodal Vision-Language
model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further
introduce a two-stage training paradigm that progressively
aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the MetaWorld and RoboTwin suite, surpassing the previous best
models by 12.4% and 6.9%, respectively, and also attains a
competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming
all baseline methods. We release code, data, and model
weights to facilitate future research on lightweight and efficient VLA models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Single fixed image display -->
<section class="section" style="padding-bottom: 1rem;">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Architecture of Evo-1</h2>
    <img src="static/images/Evo1_model.png" alt="Result Image" style="max-width: 100%; height: auto;">
  </div>
</section>

<!-- Single fixed image display -->
<section class="section" style="padding-bottom: 1rem;">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Simulation Experiments</h2>
    <img src="static/images/simulation.png" alt="Result Image" style="max-width: 100%; height: auto;">
  </div>
</section>

<!-- Single fixed image display -->
<section class="section" style="padding-bottom: 1rem;">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Real-world Experiments</h2>
    <img src="static/images/real_world.png" alt="Result Image" style="max-width: 100%; height: auto;">
  </div>
</section>
<section class="section" style="padding-bottom: 1rem;">
  <div class="container is-max-desktop">
    <ol class="content has-text-justified">
      <li>
        <strong>Pick and Place Can.</strong><br>
        
        This task requires grasping a beverage can from varying initial positions and place it into
        a white box on the table.
      </li>

      <li>
        <strong>Pour Foam from Cup.</strong><br>
        This task requires lifting a foam-filled cup from varying initial positions and rotating it to
        pour the foam into a white box.
      </li>

      <li>
        <strong>Hand Delivery.</strong><br>
        This task requires grasping a beverage
        can from varying positions and gently placing it into a
        human hand held at different locations.
      </li>

      <li>
        <strong>Can Stacking.</strong><br>
        This task requires grasping a beverage
        can and stacking it onto another with sufficient stability.
        The two cans are identical and randomly placed on the
        table.
      </li>
    </ol>
  </div>
</section>
<section class="section" style="padding-bottom: 1rem;">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Qualitative results of our model in real-world tasks</h2>
    <img src="static/images/real_world_results.png" alt="Result Image" style="max-width: 100%; height: auto;">
  </div>
</section>




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{lin2025evo1lightweightvisionlanguageactionmodel,
      title={Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment}, 
      author={Tao Lin and Yilei Zhong and Yuxin Du and Jingjing Zhang and Jiting Liu and Yinxinyu Chen and Encheng Gu and Ziyan Liu and Hongyi Cai and Yanwen Zou and Lixing Zou and Zhaoye Zhou and Gen Li and Bo Zhao},
      year={2025},
      eprint={2511.04555},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2511.04555}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
